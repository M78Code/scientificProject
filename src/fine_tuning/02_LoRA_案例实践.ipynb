{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905fa0a1-f879-4976-a7f8-da6733c86885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes: ä¸“ä¸ºé‡åŒ–è®¾è®¡çš„åº“ï¼Œé‡ç‚¹åœ¨äºå‡å°‘å¤§æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åœ¨GPUä¸Šï¼‰çš„å†…å­˜å ç”¨\n",
    "# peft: ç”¨äºå°†LoRAé€‚é…å™¨é›†æˆåˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­\n",
    "# trl: è¯¥åº“åŒ…å«ä¸€ä¸ªSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ç±»ï¼Œç”¨äºè¾…åŠ©å¾®è°ƒæ¨¡å‹\n",
    "# accelerateå’Œxformers: è¿™äº›åº“ç”¨äºæé«˜æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼Œä»è€Œä¼˜åŒ–å…¶æ€§èƒ½\n",
    "# wandb: è¯¥å·¥å…·ä½œä¸ºä¸€ä¸ªç›‘æ§å¹³å°ï¼Œç”¨äºè·Ÿè¸ªå’Œè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹\n",
    "# datasets: ä¸Hugging Faceä¸€èµ·ä½¿ç”¨ï¼Œè¯¥åº“ä¾¿äºåŠ è½½æ•°æ®é›†\n",
    "\n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os, wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc711b9-c599-4d47-bfdb-765c297f4080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583cd85d-eff4-4afb-a63e-133353d8bc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cc396-efb3-45a1-9b83-7fab3e136369",
   "metadata": {},
   "source": [
    "# 1. åŠ è½½æ¨¡å‹å’ŒTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4605d48b-a675-4042-a3ec-db5c25883f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„è®­ç»ƒæ¨¡å‹\n",
    "model_name = \"/home/leon/projects/models/Meta-Llama-3-8B/\"\n",
    "# æ•°æ®é›†åç§°\n",
    "dataset_name = \"scooterman/guanaco-llama3-1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7d4d00-8a9d-42ba-82cb-e5ef6a5b19de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a48881ac53148aeb115fbb24c028a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer\n",
    "\n",
    "# é‡åŒ–é…ç½®\n",
    "# https://huggingface.co/docs/transformers/v4.43.3/en/main_classes/quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # æ¨¡å‹å°†ä»¥4ä½é‡åŒ–æ ¼å¼åŠ è½½\n",
    "    bnb_4bit_quant_type=\"nf4\", # æŒ‡å®š4ä½é‡åŒ–çš„ç±»å‹ä¸º nf4\n",
    "    bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ•°æ®ç±»å‹\n",
    "    bnb_4bit_use_double_quant=False, # è¡¨ç¤ºä¸ä½¿ç”¨åŒé‡é‡åŒ–\n",
    ") # QLoRAä¸­çš„åŸç†\n",
    "\n",
    "# æ¨¡å‹åŠ è½½\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0} # å°†æ¨¡å‹åŠ è½½åˆ°è®¾å¤‡0ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªGPUï¼‰\n",
    ")\n",
    "\n",
    "# tokenizer åŠ è½½\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True # åœ¨ç”Ÿæˆåºåˆ—æ—¶ä¼šè‡ªåŠ¨æ·»åŠ ç»“æŸæ ‡è®°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fc0bf0-f616-4205-933d-9d8f1801f085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>user<|end_header_id|>{{Me graduÃ© hace poco de la carrera de medicina Â¿Me podrÃ­as aconsejar para conseguir rÃ¡pidamente un puesto de trabajo?}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{{Esto vale tanto para mÃ©dicos como para cualquier otra profesiÃ³n tras finalizar los estudios aniversarios y mi consejo serÃ­a preguntar a cuÃ¡ntas personas haya conocido mejor. En este caso, mi primera opciÃ³n serÃ­a hablar con otros profesionales mÃ©dicos, echar currÃ­culos en hospitales y cualquier centro de salud. En paralelo, trabajarÃ­a por mejorar mi marca personal como mÃ©dico mediante un blog o formas digitales de comunicaciÃ³n como los vÃ­deos. Y, para mejorar las posibilidades de encontrar trabajo, tambiÃ©n participarÃ­a en congresos y encuentros para conseguir mÃ¡s contactos. Y, ademÃ¡s de todo lo anterior, seguirÃ­a estudiando para presentarme a las oposiciones y ejercer la medicina en el sector pÃºblico de mi paÃ­s.}}<|eot_id|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87a668-d327-45f8-8e72-454d64cfb0bd",
   "metadata": {},
   "source": [
    "# 2. wandb é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10077622-6e9c-428b-a3db-7679b0d2d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mub313leon\u001b[0m (\u001b[33mleon-2003ub313\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/leon/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç›‘æ§\n",
    "# éœ€è¦åœ¨WandBå®˜ç½‘æ³¨å†Œè´¦å·\n",
    "wandb.login(key=\"ded693cc7edc0388564a53cb198473dc9a10e543\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8936a6-0577-415a-8492-7d97a7811dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/leon/night_task/scientificProject/src/fine_tuning/wandb/run-20240727_174700-8xhdr2ge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leon-2003ub313/test_fine_tuning/runs/8xhdr2ge' target=\"_blank\">legendary-puddle-2</a></strong> to <a href='https://wandb.ai/leon-2003ub313/test_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leon-2003ub313/test_fine_tuning' target=\"_blank\">https://wandb.ai/leon-2003ub313/test_fine_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leon-2003ub313/test_fine_tuning/runs/8xhdr2ge' target=\"_blank\">https://wandb.ai/leon-2003ub313/test_fine_tuning/runs/8xhdr2ge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"test_fine_tuning\",\n",
    "    job_type=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a666d8e9-1b8f-48f1-8719-976b88309444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—è®­ç»ƒå‚æ•°é‡\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params=0\n",
    "    all_param=0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "        print(f\"è®­ç»ƒå‚æ•°é‡: {trainable_params} || æ€»çš„å‚æ•°é‡: {all_param} || è®­ç»ƒå‚æ•°é‡å æ¯”%: {100*(trainable_params/all_param):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2b4fd-fe12-4083-8aa6-4648cc654fd2",
   "metadata": {},
   "source": [
    "# 3. LoRAä¸è®­ç»ƒè¶…å‚é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3163472a-eee8-4a2f-a568-259f5c0cc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha=16, # å°æŠ€å·§ï¼ŒæŠŠaå€¼è®¾ç½®æˆrankå€¼çš„ä¸¤å€\n",
    "    # scaling = alpha / r # LoRAæƒé‡çš„å€¼è¶Šå¤§ï¼Œå½±å“å°±è¶Šå¤§\n",
    "    # weight += (lora_B @ lora_A)*scaling\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    # [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"embed_tokens\",\"lm_head\"]\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647f90c9-a876-4c02-81c8-43270c06e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒè¶…å‚\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"/home/leon/projects/models/autodl-tmp/\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¸º2ï¼Œå³æ¯2æ­¥æ›´æ–°ä¸€æ¬¡æ¢¯åº¦ï¼Œæœ‰è‚‹äºåœ¨æ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹ä½¿ç”¨è¾ƒå¤§çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ã€‚\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=50, # æ¯100æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    logging_steps=30,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001, # æƒé‡è¡°å‡ç³»æ•°ï¼Œç”¨äºL2æ­£åˆ™åŒ–ï¼Œå¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3, # æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼Œç”¨äºæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œ\n",
    "    max_steps=-1, # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼1ï¼Œè¡¨ç¤ºæ²¡æœ‰é™åˆ¶ï¼Œ\n",
    "    warmup_ratio=0.3, # é¢„çƒ­é˜¶æ®µçš„æ¯”ä¾‹ï¼Œåœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œå­¦ä¹ ç‡ä¼šé€æ¸å‡é«˜ï¼Œé¢„çƒ­æ¯”ä¾‹ä¸º0.3è¡¨ç¤ºå‰30ï¼…çš„è®­ç»ƒæ­¥éª¤ç”¨äºé¢„çƒ­ã€‚\n",
    "    group_by_length=True, # æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ï¼Œ\n",
    "    lr_scheduler_type=\"linear\", # è¡¨ç¤ºä½¿ç”¨çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ã€‚\n",
    "    report_to=\"wandb\", # tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e9a39-5b0d-4b2a-aaad-0890e8dee484",
   "metadata": {},
   "source": [
    "# 4. æ¨¡å‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "164a1dd5-5f47-4e03-b943-e9cde2855724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51802e42-3148-4754-8b76-d9b2ff224334",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.82 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    967\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    968\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    969\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         cache_position,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:732\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    735\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:215\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/peft/tuners/lora/bnb.py:458\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.82 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "144bdd58-1f34-40be-aedb-9546f5ec548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå‚æ•°é‡: 0 || æ€»çš„å‚æ•°é‡: 525336576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.00\n",
      "è®­ç»ƒå‚æ•°é‡: 0 || æ€»çš„å‚æ•°é‡: 533725184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.00\n",
      "è®­ç»ƒå‚æ•°é‡: 32768 || æ€»çš„å‚æ•°é‡: 533757952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.01\n",
      "è®­ç»ƒå‚æ•°é‡: 65536 || æ€»çš„å‚æ•°é‡: 533790720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.01\n",
      "è®­ç»ƒå‚æ•°é‡: 65536 || æ€»çš„å‚æ•°é‡: 535887872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.01\n",
      "è®­ç»ƒå‚æ•°é‡: 98304 || æ€»çš„å‚æ•°é‡: 535920640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.02\n",
      "è®­ç»ƒå‚æ•°é‡: 106496 || æ€»çš„å‚æ•°é‡: 535928832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.02\n",
      "è®­ç»ƒå‚æ•°é‡: 106496 || æ€»çš„å‚æ•°é‡: 538025984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.02\n",
      "è®­ç»ƒå‚æ•°é‡: 139264 || æ€»çš„å‚æ•°é‡: 538058752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.03\n",
      "è®­ç»ƒå‚æ•°é‡: 147456 || æ€»çš„å‚æ•°é‡: 538066944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.03\n",
      "è®­ç»ƒå‚æ•°é‡: 147456 || æ€»çš„å‚æ•°é‡: 546455552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.03\n",
      "è®­ç»ƒå‚æ•°é‡: 180224 || æ€»çš„å‚æ•°é‡: 546488320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.03\n",
      "è®­ç»ƒå‚æ•°é‡: 212992 || æ€»çš„å‚æ•°é‡: 546521088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.04\n",
      "è®­ç»ƒå‚æ•°é‡: 212992 || æ€»çš„å‚æ•°é‡: 575881216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.04\n",
      "è®­ç»ƒå‚æ•°é‡: 245760 || æ€»çš„å‚æ•°é‡: 575913984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.04\n",
      "è®­ç»ƒå‚æ•°é‡: 360448 || æ€»çš„å‚æ•°é‡: 576028672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.06\n",
      "è®­ç»ƒå‚æ•°é‡: 360448 || æ€»çš„å‚æ•°é‡: 605388800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.06\n",
      "è®­ç»ƒå‚æ•°é‡: 393216 || æ€»çš„å‚æ•°é‡: 605421568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.06\n",
      "è®­ç»ƒå‚æ•°é‡: 507904 || æ€»çš„å‚æ•°é‡: 605536256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 507904 || æ€»çš„å‚æ•°é‡: 634896384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 507904 || æ€»çš„å‚æ•°é‡: 634900480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 507904 || æ€»çš„å‚æ•°é‡: 634904576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 507904 || æ€»çš„å‚æ•°é‡: 643293184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 540672 || æ€»çš„å‚æ•°é‡: 643325952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.08\n",
      "è®­ç»ƒå‚æ•°é‡: 573440 || æ€»çš„å‚æ•°é‡: 643358720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.09\n",
      "è®­ç»ƒå‚æ•°é‡: 573440 || æ€»çš„å‚æ•°é‡: 645455872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.09\n",
      "è®­ç»ƒå‚æ•°é‡: 606208 || æ€»çš„å‚æ•°é‡: 645488640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.09\n",
      "è®­ç»ƒå‚æ•°é‡: 614400 || æ€»çš„å‚æ•°é‡: 645496832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.10\n",
      "è®­ç»ƒå‚æ•°é‡: 614400 || æ€»çš„å‚æ•°é‡: 647593984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.09\n",
      "è®­ç»ƒå‚æ•°é‡: 647168 || æ€»çš„å‚æ•°é‡: 647626752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.10\n",
      "è®­ç»ƒå‚æ•°é‡: 655360 || æ€»çš„å‚æ•°é‡: 647634944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.10\n",
      "è®­ç»ƒå‚æ•°é‡: 655360 || æ€»çš„å‚æ•°é‡: 656023552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.10\n",
      "è®­ç»ƒå‚æ•°é‡: 688128 || æ€»çš„å‚æ•°é‡: 656056320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.10\n",
      "è®­ç»ƒå‚æ•°é‡: 720896 || æ€»çš„å‚æ•°é‡: 656089088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.11\n",
      "è®­ç»ƒå‚æ•°é‡: 720896 || æ€»çš„å‚æ•°é‡: 685449216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.11\n",
      "è®­ç»ƒå‚æ•°é‡: 753664 || æ€»çš„å‚æ•°é‡: 685481984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.11\n",
      "è®­ç»ƒå‚æ•°é‡: 868352 || æ€»çš„å‚æ•°é‡: 685596672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.13\n",
      "è®­ç»ƒå‚æ•°é‡: 868352 || æ€»çš„å‚æ•°é‡: 714956800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.12\n",
      "è®­ç»ƒå‚æ•°é‡: 901120 || æ€»çš„å‚æ•°é‡: 714989568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.13\n",
      "è®­ç»ƒå‚æ•°é‡: 1015808 || æ€»çš„å‚æ•°é‡: 715104256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1015808 || æ€»çš„å‚æ•°é‡: 744464384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1015808 || æ€»çš„å‚æ•°é‡: 744468480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1015808 || æ€»çš„å‚æ•°é‡: 744472576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1015808 || æ€»çš„å‚æ•°é‡: 752861184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.13\n",
      "è®­ç»ƒå‚æ•°é‡: 1048576 || æ€»çš„å‚æ•°é‡: 752893952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1081344 || æ€»çš„å‚æ•°é‡: 752926720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1081344 || æ€»çš„å‚æ•°é‡: 755023872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.14\n",
      "è®­ç»ƒå‚æ•°é‡: 1114112 || æ€»çš„å‚æ•°é‡: 755056640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1122304 || æ€»çš„å‚æ•°é‡: 755064832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1122304 || æ€»çš„å‚æ•°é‡: 757161984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1155072 || æ€»çš„å‚æ•°é‡: 757194752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1163264 || æ€»çš„å‚æ•°é‡: 757202944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1163264 || æ€»çš„å‚æ•°é‡: 765591552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1196032 || æ€»çš„å‚æ•°é‡: 765624320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.16\n",
      "è®­ç»ƒå‚æ•°é‡: 1228800 || æ€»çš„å‚æ•°é‡: 765657088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.16\n",
      "è®­ç»ƒå‚æ•°é‡: 1228800 || æ€»çš„å‚æ•°é‡: 795017216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.15\n",
      "è®­ç»ƒå‚æ•°é‡: 1261568 || æ€»çš„å‚æ•°é‡: 795049984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.16\n",
      "è®­ç»ƒå‚æ•°é‡: 1376256 || æ€»çš„å‚æ•°é‡: 795164672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.17\n",
      "è®­ç»ƒå‚æ•°é‡: 1376256 || æ€»çš„å‚æ•°é‡: 824524800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.17\n",
      "è®­ç»ƒå‚æ•°é‡: 1409024 || æ€»çš„å‚æ•°é‡: 824557568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.17\n",
      "è®­ç»ƒå‚æ•°é‡: 1523712 || æ€»çš„å‚æ•°é‡: 824672256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1523712 || æ€»çš„å‚æ•°é‡: 854032384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1523712 || æ€»çš„å‚æ•°é‡: 854036480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1523712 || æ€»çš„å‚æ•°é‡: 854040576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1523712 || æ€»çš„å‚æ•°é‡: 862429184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1556480 || æ€»çš„å‚æ•°é‡: 862461952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1589248 || æ€»çš„å‚æ•°é‡: 862494720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1589248 || æ€»çš„å‚æ•°é‡: 864591872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.18\n",
      "è®­ç»ƒå‚æ•°é‡: 1622016 || æ€»çš„å‚æ•°é‡: 864624640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1630208 || æ€»çš„å‚æ•°é‡: 864632832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1630208 || æ€»çš„å‚æ•°é‡: 866729984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1662976 || æ€»çš„å‚æ•°é‡: 866762752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1671168 || æ€»çš„å‚æ•°é‡: 866770944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1671168 || æ€»çš„å‚æ•°é‡: 875159552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1703936 || æ€»çš„å‚æ•°é‡: 875192320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1736704 || æ€»çš„å‚æ•°é‡: 875225088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.20\n",
      "è®­ç»ƒå‚æ•°é‡: 1736704 || æ€»çš„å‚æ•°é‡: 904585216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.19\n",
      "è®­ç»ƒå‚æ•°é‡: 1769472 || æ€»çš„å‚æ•°é‡: 904617984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.20\n",
      "è®­ç»ƒå‚æ•°é‡: 1884160 || æ€»çš„å‚æ•°é‡: 904732672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 1884160 || æ€»çš„å‚æ•°é‡: 934092800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.20\n",
      "è®­ç»ƒå‚æ•°é‡: 1916928 || æ€»çš„å‚æ•°é‡: 934125568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2031616 || æ€»çš„å‚æ•°é‡: 934240256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2031616 || æ€»çš„å‚æ•°é‡: 963600384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2031616 || æ€»çš„å‚æ•°é‡: 963604480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2031616 || æ€»çš„å‚æ•°é‡: 963608576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2031616 || æ€»çš„å‚æ•°é‡: 971997184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2064384 || æ€»çš„å‚æ•°é‡: 972029952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.21\n",
      "è®­ç»ƒå‚æ•°é‡: 2097152 || æ€»çš„å‚æ•°é‡: 972062720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2097152 || æ€»çš„å‚æ•°é‡: 974159872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2129920 || æ€»çš„å‚æ•°é‡: 974192640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2138112 || æ€»çš„å‚æ•°é‡: 974200832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2138112 || æ€»çš„å‚æ•°é‡: 976297984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2170880 || æ€»çš„å‚æ•°é‡: 976330752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2179072 || æ€»çš„å‚æ•°é‡: 976338944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2179072 || æ€»çš„å‚æ•°é‡: 984727552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2211840 || æ€»çš„å‚æ•°é‡: 984760320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2244608 || æ€»çš„å‚æ•°é‡: 984793088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.23\n",
      "è®­ç»ƒå‚æ•°é‡: 2244608 || æ€»çš„å‚æ•°é‡: 1014153216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2277376 || æ€»çš„å‚æ•°é‡: 1014185984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.22\n",
      "è®­ç»ƒå‚æ•°é‡: 2392064 || æ€»çš„å‚æ•°é‡: 1014300672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2392064 || æ€»çš„å‚æ•°é‡: 1043660800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.23\n",
      "è®­ç»ƒå‚æ•°é‡: 2424832 || æ€»çš„å‚æ•°é‡: 1043693568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.23\n",
      "è®­ç»ƒå‚æ•°é‡: 2539520 || æ€»çš„å‚æ•°é‡: 1043808256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2539520 || æ€»çš„å‚æ•°é‡: 1073168384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2539520 || æ€»çš„å‚æ•°é‡: 1073172480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2539520 || æ€»çš„å‚æ•°é‡: 1073176576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2539520 || æ€»çš„å‚æ•°é‡: 1081565184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.23\n",
      "è®­ç»ƒå‚æ•°é‡: 2572288 || æ€»çš„å‚æ•°é‡: 1081597952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2605056 || æ€»çš„å‚æ•°é‡: 1081630720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2605056 || æ€»çš„å‚æ•°é‡: 1083727872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2637824 || æ€»çš„å‚æ•°é‡: 1083760640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2646016 || æ€»çš„å‚æ•°é‡: 1083768832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2646016 || æ€»çš„å‚æ•°é‡: 1085865984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2678784 || æ€»çš„å‚æ•°é‡: 1085898752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2686976 || æ€»çš„å‚æ•°é‡: 1085906944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2686976 || æ€»çš„å‚æ•°é‡: 1094295552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2719744 || æ€»çš„å‚æ•°é‡: 1094328320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2752512 || æ€»çš„å‚æ•°é‡: 1094361088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2752512 || æ€»çš„å‚æ•°é‡: 1123721216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.24\n",
      "è®­ç»ƒå‚æ•°é‡: 2785280 || æ€»çš„å‚æ•°é‡: 1123753984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2899968 || æ€»çš„å‚æ•°é‡: 1123868672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 2899968 || æ€»çš„å‚æ•°é‡: 1153228800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 2932736 || æ€»çš„å‚æ•°é‡: 1153261568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.25\n",
      "è®­ç»ƒå‚æ•°é‡: 3047424 || æ€»çš„å‚æ•°é‡: 1153376256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3047424 || æ€»çš„å‚æ•°é‡: 1182736384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3047424 || æ€»çš„å‚æ•°é‡: 1182740480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3047424 || æ€»çš„å‚æ•°é‡: 1182744576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3047424 || æ€»çš„å‚æ•°é‡: 1191133184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3080192 || æ€»çš„å‚æ•°é‡: 1191165952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3112960 || æ€»çš„å‚æ•°é‡: 1191198720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3112960 || æ€»çš„å‚æ•°é‡: 1193295872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3145728 || æ€»çš„å‚æ•°é‡: 1193328640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3153920 || æ€»çš„å‚æ•°é‡: 1193336832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3153920 || æ€»çš„å‚æ•°é‡: 1195433984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3186688 || æ€»çš„å‚æ•°é‡: 1195466752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3194880 || æ€»çš„å‚æ•°é‡: 1195474944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3194880 || æ€»çš„å‚æ•°é‡: 1203863552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3227648 || æ€»çš„å‚æ•°é‡: 1203896320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3260416 || æ€»çš„å‚æ•°é‡: 1203929088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3260416 || æ€»çš„å‚æ•°é‡: 1233289216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.26\n",
      "è®­ç»ƒå‚æ•°é‡: 3293184 || æ€»çš„å‚æ•°é‡: 1233321984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3407872 || æ€»çš„å‚æ•°é‡: 1233436672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3407872 || æ€»çš„å‚æ•°é‡: 1262796800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3440640 || æ€»çš„å‚æ•°é‡: 1262829568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3555328 || æ€»çš„å‚æ•°é‡: 1262944256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3555328 || æ€»çš„å‚æ•°é‡: 1292304384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3555328 || æ€»çš„å‚æ•°é‡: 1292308480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3555328 || æ€»çš„å‚æ•°é‡: 1292312576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3555328 || æ€»çš„å‚æ•°é‡: 1300701184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.27\n",
      "è®­ç»ƒå‚æ•°é‡: 3588096 || æ€»çš„å‚æ•°é‡: 1300733952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3620864 || æ€»çš„å‚æ•°é‡: 1300766720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3620864 || æ€»çš„å‚æ•°é‡: 1302863872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3653632 || æ€»çš„å‚æ•°é‡: 1302896640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3661824 || æ€»çš„å‚æ•°é‡: 1302904832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3661824 || æ€»çš„å‚æ•°é‡: 1305001984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3694592 || æ€»çš„å‚æ•°é‡: 1305034752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3702784 || æ€»çš„å‚æ•°é‡: 1305042944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3702784 || æ€»çš„å‚æ•°é‡: 1313431552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3735552 || æ€»çš„å‚æ•°é‡: 1313464320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3768320 || æ€»çš„å‚æ•°é‡: 1313497088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 3768320 || æ€»çš„å‚æ•°é‡: 1342857216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3801088 || æ€»çš„å‚æ•°é‡: 1342889984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.28\n",
      "è®­ç»ƒå‚æ•°é‡: 3915776 || æ€»çš„å‚æ•°é‡: 1343004672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 3915776 || æ€»çš„å‚æ•°é‡: 1372364800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 3948544 || æ€»çš„å‚æ•°é‡: 1372397568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4063232 || æ€»çš„å‚æ•°é‡: 1372512256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4063232 || æ€»çš„å‚æ•°é‡: 1401872384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4063232 || æ€»çš„å‚æ•°é‡: 1401876480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4063232 || æ€»çš„å‚æ•°é‡: 1401880576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4063232 || æ€»çš„å‚æ•°é‡: 1410269184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4096000 || æ€»çš„å‚æ•°é‡: 1410301952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4128768 || æ€»çš„å‚æ•°é‡: 1410334720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4128768 || æ€»çš„å‚æ•°é‡: 1412431872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4161536 || æ€»çš„å‚æ•°é‡: 1412464640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4169728 || æ€»çš„å‚æ•°é‡: 1412472832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4169728 || æ€»çš„å‚æ•°é‡: 1414569984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4202496 || æ€»çš„å‚æ•°é‡: 1414602752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4210688 || æ€»çš„å‚æ•°é‡: 1414610944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4210688 || æ€»çš„å‚æ•°é‡: 1422999552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4243456 || æ€»çš„å‚æ•°é‡: 1423032320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4276224 || æ€»çš„å‚æ•°é‡: 1423065088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4276224 || æ€»çš„å‚æ•°é‡: 1452425216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.29\n",
      "è®­ç»ƒå‚æ•°é‡: 4308992 || æ€»çš„å‚æ•°é‡: 1452457984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4423680 || æ€»çš„å‚æ•°é‡: 1452572672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4423680 || æ€»çš„å‚æ•°é‡: 1481932800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4456448 || æ€»çš„å‚æ•°é‡: 1481965568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4571136 || æ€»çš„å‚æ•°é‡: 1482080256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4571136 || æ€»çš„å‚æ•°é‡: 1511440384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4571136 || æ€»çš„å‚æ•°é‡: 1511444480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4571136 || æ€»çš„å‚æ•°é‡: 1511448576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4571136 || æ€»çš„å‚æ•°é‡: 1519837184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4603904 || æ€»çš„å‚æ•°é‡: 1519869952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4636672 || æ€»çš„å‚æ•°é‡: 1519902720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4636672 || æ€»çš„å‚æ•°é‡: 1521999872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.30\n",
      "è®­ç»ƒå‚æ•°é‡: 4669440 || æ€»çš„å‚æ•°é‡: 1522032640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4677632 || æ€»çš„å‚æ•°é‡: 1522040832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4677632 || æ€»çš„å‚æ•°é‡: 1524137984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4710400 || æ€»çš„å‚æ•°é‡: 1524170752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4718592 || æ€»çš„å‚æ•°é‡: 1524178944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4718592 || æ€»çš„å‚æ•°é‡: 1532567552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4751360 || æ€»çš„å‚æ•°é‡: 1532600320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4784128 || æ€»çš„å‚æ•°é‡: 1532633088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4784128 || æ€»çš„å‚æ•°é‡: 1561993216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4816896 || æ€»çš„å‚æ•°é‡: 1562025984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4931584 || æ€»çš„å‚æ•°é‡: 1562140672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 4931584 || æ€»çš„å‚æ•°é‡: 1591500800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 4964352 || æ€»çš„å‚æ•°é‡: 1591533568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5079040 || æ€»çš„å‚æ•°é‡: 1591648256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5079040 || æ€»çš„å‚æ•°é‡: 1621008384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5079040 || æ€»çš„å‚æ•°é‡: 1621012480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5079040 || æ€»çš„å‚æ•°é‡: 1621016576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5079040 || æ€»çš„å‚æ•°é‡: 1629405184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5111808 || æ€»çš„å‚æ•°é‡: 1629437952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.31\n",
      "è®­ç»ƒå‚æ•°é‡: 5144576 || æ€»çš„å‚æ•°é‡: 1629470720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5144576 || æ€»çš„å‚æ•°é‡: 1631567872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5177344 || æ€»çš„å‚æ•°é‡: 1631600640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5185536 || æ€»çš„å‚æ•°é‡: 1631608832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5185536 || æ€»çš„å‚æ•°é‡: 1633705984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5218304 || æ€»çš„å‚æ•°é‡: 1633738752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5226496 || æ€»çš„å‚æ•°é‡: 1633746944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5226496 || æ€»çš„å‚æ•°é‡: 1642135552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5259264 || æ€»çš„å‚æ•°é‡: 1642168320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5292032 || æ€»çš„å‚æ•°é‡: 1642201088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5292032 || æ€»çš„å‚æ•°é‡: 1671561216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5324800 || æ€»çš„å‚æ•°é‡: 1671593984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5439488 || æ€»çš„å‚æ•°é‡: 1671708672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5439488 || æ€»çš„å‚æ•°é‡: 1701068800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5472256 || æ€»çš„å‚æ•°é‡: 1701101568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5586944 || æ€»çš„å‚æ•°é‡: 1701216256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5586944 || æ€»çš„å‚æ•°é‡: 1730576384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5586944 || æ€»çš„å‚æ•°é‡: 1730580480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5586944 || æ€»çš„å‚æ•°é‡: 1730584576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5586944 || æ€»çš„å‚æ•°é‡: 1738973184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5619712 || æ€»çš„å‚æ•°é‡: 1739005952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5652480 || æ€»çš„å‚æ•°é‡: 1739038720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5652480 || æ€»çš„å‚æ•°é‡: 1741135872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.32\n",
      "è®­ç»ƒå‚æ•°é‡: 5685248 || æ€»çš„å‚æ•°é‡: 1741168640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5693440 || æ€»çš„å‚æ•°é‡: 1741176832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5693440 || æ€»çš„å‚æ•°é‡: 1743273984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5726208 || æ€»çš„å‚æ•°é‡: 1743306752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5734400 || æ€»çš„å‚æ•°é‡: 1743314944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5734400 || æ€»çš„å‚æ•°é‡: 1751703552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5767168 || æ€»çš„å‚æ•°é‡: 1751736320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5799936 || æ€»çš„å‚æ•°é‡: 1751769088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5799936 || æ€»çš„å‚æ•°é‡: 1781129216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5832704 || æ€»çš„å‚æ•°é‡: 1781161984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5947392 || æ€»çš„å‚æ•°é‡: 1781276672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5947392 || æ€»çš„å‚æ•°é‡: 1810636800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 5980160 || æ€»çš„å‚æ•°é‡: 1810669568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6094848 || æ€»çš„å‚æ•°é‡: 1810784256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6094848 || æ€»çš„å‚æ•°é‡: 1840144384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6094848 || æ€»çš„å‚æ•°é‡: 1840148480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6094848 || æ€»çš„å‚æ•°é‡: 1840152576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6094848 || æ€»çš„å‚æ•°é‡: 1848541184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6127616 || æ€»çš„å‚æ•°é‡: 1848573952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6160384 || æ€»çš„å‚æ•°é‡: 1848606720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6160384 || æ€»çš„å‚æ•°é‡: 1850703872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6193152 || æ€»çš„å‚æ•°é‡: 1850736640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6201344 || æ€»çš„å‚æ•°é‡: 1850744832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6201344 || æ€»çš„å‚æ•°é‡: 1852841984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6234112 || æ€»çš„å‚æ•°é‡: 1852874752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6242304 || æ€»çš„å‚æ•°é‡: 1852882944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6242304 || æ€»çš„å‚æ•°é‡: 1861271552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6275072 || æ€»çš„å‚æ•°é‡: 1861304320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6307840 || æ€»çš„å‚æ•°é‡: 1861337088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6307840 || æ€»çš„å‚æ•°é‡: 1890697216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.33\n",
      "è®­ç»ƒå‚æ•°é‡: 6340608 || æ€»çš„å‚æ•°é‡: 1890729984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6455296 || æ€»çš„å‚æ•°é‡: 1890844672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6455296 || æ€»çš„å‚æ•°é‡: 1920204800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6488064 || æ€»çš„å‚æ•°é‡: 1920237568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6602752 || æ€»çš„å‚æ•°é‡: 1920352256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6602752 || æ€»çš„å‚æ•°é‡: 1949712384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6602752 || æ€»çš„å‚æ•°é‡: 1949716480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6602752 || æ€»çš„å‚æ•°é‡: 1949720576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6602752 || æ€»çš„å‚æ•°é‡: 1958109184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6635520 || æ€»çš„å‚æ•°é‡: 1958141952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6668288 || æ€»çš„å‚æ•°é‡: 1958174720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6668288 || æ€»çš„å‚æ•°é‡: 1960271872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6701056 || æ€»çš„å‚æ•°é‡: 1960304640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6709248 || æ€»çš„å‚æ•°é‡: 1960312832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6709248 || æ€»çš„å‚æ•°é‡: 1962409984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6742016 || æ€»çš„å‚æ•°é‡: 1962442752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6750208 || æ€»çš„å‚æ•°é‡: 1962450944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6750208 || æ€»çš„å‚æ•°é‡: 1970839552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6782976 || æ€»çš„å‚æ•°é‡: 1970872320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6815744 || æ€»çš„å‚æ•°é‡: 1970905088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 6815744 || æ€»çš„å‚æ•°é‡: 2000265216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6848512 || æ€»çš„å‚æ•°é‡: 2000297984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6963200 || æ€»çš„å‚æ•°é‡: 2000412672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 6963200 || æ€»çš„å‚æ•°é‡: 2029772800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 6995968 || æ€»çš„å‚æ•°é‡: 2029805568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 7110656 || æ€»çš„å‚æ•°é‡: 2029920256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7110656 || æ€»çš„å‚æ•°é‡: 2059280384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7110656 || æ€»çš„å‚æ•°é‡: 2059284480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7110656 || æ€»çš„å‚æ•°é‡: 2059288576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7110656 || æ€»çš„å‚æ•°é‡: 2067677184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.34\n",
      "è®­ç»ƒå‚æ•°é‡: 7143424 || æ€»çš„å‚æ•°é‡: 2067709952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7176192 || æ€»çš„å‚æ•°é‡: 2067742720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7176192 || æ€»çš„å‚æ•°é‡: 2069839872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7208960 || æ€»çš„å‚æ•°é‡: 2069872640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7217152 || æ€»çš„å‚æ•°é‡: 2069880832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7217152 || æ€»çš„å‚æ•°é‡: 2071977984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7249920 || æ€»çš„å‚æ•°é‡: 2072010752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7258112 || æ€»çš„å‚æ•°é‡: 2072018944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7258112 || æ€»çš„å‚æ•°é‡: 2080407552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7290880 || æ€»çš„å‚æ•°é‡: 2080440320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7323648 || æ€»çš„å‚æ•°é‡: 2080473088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7323648 || æ€»çš„å‚æ•°é‡: 2109833216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7356416 || æ€»çš„å‚æ•°é‡: 2109865984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7471104 || æ€»çš„å‚æ•°é‡: 2109980672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7471104 || æ€»çš„å‚æ•°é‡: 2139340800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7503872 || æ€»çš„å‚æ•°é‡: 2139373568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7618560 || æ€»çš„å‚æ•°é‡: 2139488256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7618560 || æ€»çš„å‚æ•°é‡: 2168848384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7618560 || æ€»çš„å‚æ•°é‡: 2168852480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7618560 || æ€»çš„å‚æ•°é‡: 2168856576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7618560 || æ€»çš„å‚æ•°é‡: 2177245184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7651328 || æ€»çš„å‚æ•°é‡: 2177277952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7684096 || æ€»çš„å‚æ•°é‡: 2177310720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7684096 || æ€»çš„å‚æ•°é‡: 2179407872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7716864 || æ€»çš„å‚æ•°é‡: 2179440640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7725056 || æ€»çš„å‚æ•°é‡: 2179448832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7725056 || æ€»çš„å‚æ•°é‡: 2181545984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7757824 || æ€»çš„å‚æ•°é‡: 2181578752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7766016 || æ€»çš„å‚æ•°é‡: 2181586944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7766016 || æ€»çš„å‚æ•°é‡: 2189975552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7798784 || æ€»çš„å‚æ•°é‡: 2190008320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7831552 || æ€»çš„å‚æ•°é‡: 2190041088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7831552 || æ€»çš„å‚æ•°é‡: 2219401216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7864320 || æ€»çš„å‚æ•°é‡: 2219433984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 7979008 || æ€»çš„å‚æ•°é‡: 2219548672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 7979008 || æ€»çš„å‚æ•°é‡: 2248908800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.35\n",
      "è®­ç»ƒå‚æ•°é‡: 8011776 || æ€»çš„å‚æ•°é‡: 2248941568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8126464 || æ€»çš„å‚æ•°é‡: 2249056256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8126464 || æ€»çš„å‚æ•°é‡: 2278416384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8126464 || æ€»çš„å‚æ•°é‡: 2278420480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8126464 || æ€»çš„å‚æ•°é‡: 2278424576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8126464 || æ€»çš„å‚æ•°é‡: 2286813184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8159232 || æ€»çš„å‚æ•°é‡: 2286845952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8192000 || æ€»çš„å‚æ•°é‡: 2286878720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8192000 || æ€»çš„å‚æ•°é‡: 2288975872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8224768 || æ€»çš„å‚æ•°é‡: 2289008640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8232960 || æ€»çš„å‚æ•°é‡: 2289016832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8232960 || æ€»çš„å‚æ•°é‡: 2291113984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8265728 || æ€»çš„å‚æ•°é‡: 2291146752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8273920 || æ€»çš„å‚æ•°é‡: 2291154944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8273920 || æ€»çš„å‚æ•°é‡: 2299543552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8306688 || æ€»çš„å‚æ•°é‡: 2299576320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8339456 || æ€»çš„å‚æ•°é‡: 2299609088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8339456 || æ€»çš„å‚æ•°é‡: 2328969216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8372224 || æ€»çš„å‚æ•°é‡: 2329001984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8486912 || æ€»çš„å‚æ•°é‡: 2329116672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8486912 || æ€»çš„å‚æ•°é‡: 2358476800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8519680 || æ€»çš„å‚æ•°é‡: 2358509568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8634368 || æ€»çš„å‚æ•°é‡: 2358624256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8634368 || æ€»çš„å‚æ•°é‡: 2387984384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8634368 || æ€»çš„å‚æ•°é‡: 2387988480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8634368 || æ€»çš„å‚æ•°é‡: 2387992576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8634368 || æ€»çš„å‚æ•°é‡: 2396381184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8667136 || æ€»çš„å‚æ•°é‡: 2396413952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8699904 || æ€»çš„å‚æ•°é‡: 2396446720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8699904 || æ€»çš„å‚æ•°é‡: 2398543872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8732672 || æ€»çš„å‚æ•°é‡: 2398576640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8740864 || æ€»çš„å‚æ•°é‡: 2398584832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8740864 || æ€»çš„å‚æ•°é‡: 2400681984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8773632 || æ€»çš„å‚æ•°é‡: 2400714752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8781824 || æ€»çš„å‚æ•°é‡: 2400722944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8781824 || æ€»çš„å‚æ•°é‡: 2409111552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8814592 || æ€»çš„å‚æ•°é‡: 2409144320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8847360 || æ€»çš„å‚æ•°é‡: 2409177088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8847360 || æ€»çš„å‚æ•°é‡: 2438537216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8880128 || æ€»çš„å‚æ•°é‡: 2438569984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 8994816 || æ€»çš„å‚æ•°é‡: 2438684672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 8994816 || æ€»çš„å‚æ•°é‡: 2468044800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 9027584 || æ€»çš„å‚æ•°é‡: 2468077568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9142272 || æ€»çš„å‚æ•°é‡: 2468192256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9142272 || æ€»çš„å‚æ•°é‡: 2497552384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9142272 || æ€»çš„å‚æ•°é‡: 2497556480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9142272 || æ€»çš„å‚æ•°é‡: 2497560576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9142272 || æ€»çš„å‚æ•°é‡: 2505949184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n",
      "è®­ç»ƒå‚æ•°é‡: 9175040 || æ€»çš„å‚æ•°é‡: 2505981952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9207808 || æ€»çš„å‚æ•°é‡: 2506014720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9207808 || æ€»çš„å‚æ•°é‡: 2508111872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9240576 || æ€»çš„å‚æ•°é‡: 2508144640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9248768 || æ€»çš„å‚æ•°é‡: 2508152832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9248768 || æ€»çš„å‚æ•°é‡: 2510249984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9281536 || æ€»çš„å‚æ•°é‡: 2510282752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9289728 || æ€»çš„å‚æ•°é‡: 2510290944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9289728 || æ€»çš„å‚æ•°é‡: 2518679552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9322496 || æ€»çš„å‚æ•°é‡: 2518712320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9355264 || æ€»çš„å‚æ•°é‡: 2518745088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9355264 || æ€»çš„å‚æ•°é‡: 2548105216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9388032 || æ€»çš„å‚æ•°é‡: 2548137984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9502720 || æ€»çš„å‚æ•°é‡: 2548252672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9502720 || æ€»çš„å‚æ•°é‡: 2577612800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9535488 || æ€»çš„å‚æ•°é‡: 2577645568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9650176 || æ€»çš„å‚æ•°é‡: 2577760256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9650176 || æ€»çš„å‚æ•°é‡: 2607120384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9650176 || æ€»çš„å‚æ•°é‡: 2607124480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9650176 || æ€»çš„å‚æ•°é‡: 2607128576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9650176 || æ€»çš„å‚æ•°é‡: 2615517184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9682944 || æ€»çš„å‚æ•°é‡: 2615549952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9715712 || æ€»çš„å‚æ•°é‡: 2615582720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9715712 || æ€»çš„å‚æ•°é‡: 2617679872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9748480 || æ€»çš„å‚æ•°é‡: 2617712640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9756672 || æ€»çš„å‚æ•°é‡: 2617720832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9756672 || æ€»çš„å‚æ•°é‡: 2619817984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9789440 || æ€»çš„å‚æ•°é‡: 2619850752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9797632 || æ€»çš„å‚æ•°é‡: 2619858944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9797632 || æ€»çš„å‚æ•°é‡: 2628247552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9830400 || æ€»çš„å‚æ•°é‡: 2628280320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9863168 || æ€»çš„å‚æ•°é‡: 2628313088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 9863168 || æ€»çš„å‚æ•°é‡: 2657673216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 9895936 || æ€»çš„å‚æ•°é‡: 2657705984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10010624 || æ€»çš„å‚æ•°é‡: 2657820672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10010624 || æ€»çš„å‚æ•°é‡: 2687180800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10043392 || æ€»çš„å‚æ•°é‡: 2687213568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10158080 || æ€»çš„å‚æ•°é‡: 2687328256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10158080 || æ€»çš„å‚æ•°é‡: 2716688384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10158080 || æ€»çš„å‚æ•°é‡: 2716692480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10158080 || æ€»çš„å‚æ•°é‡: 2716696576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10158080 || æ€»çš„å‚æ•°é‡: 2725085184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10190848 || æ€»çš„å‚æ•°é‡: 2725117952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10223616 || æ€»çš„å‚æ•°é‡: 2725150720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10223616 || æ€»çš„å‚æ•°é‡: 2727247872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10256384 || æ€»çš„å‚æ•°é‡: 2727280640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10264576 || æ€»çš„å‚æ•°é‡: 2727288832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10264576 || æ€»çš„å‚æ•°é‡: 2729385984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10297344 || æ€»çš„å‚æ•°é‡: 2729418752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10305536 || æ€»çš„å‚æ•°é‡: 2729426944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10305536 || æ€»çš„å‚æ•°é‡: 2737815552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10338304 || æ€»çš„å‚æ•°é‡: 2737848320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10371072 || æ€»çš„å‚æ•°é‡: 2737881088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10371072 || æ€»çš„å‚æ•°é‡: 2767241216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.37\n",
      "è®­ç»ƒå‚æ•°é‡: 10403840 || æ€»çš„å‚æ•°é‡: 2767273984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10518528 || æ€»çš„å‚æ•°é‡: 2767388672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10518528 || æ€»çš„å‚æ•°é‡: 2796748800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10551296 || æ€»çš„å‚æ•°é‡: 2796781568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10665984 || æ€»çš„å‚æ•°é‡: 2796896256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10665984 || æ€»çš„å‚æ•°é‡: 2826256384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10665984 || æ€»çš„å‚æ•°é‡: 2826260480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10665984 || æ€»çš„å‚æ•°é‡: 2826264576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10665984 || æ€»çš„å‚æ•°é‡: 2834653184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10698752 || æ€»çš„å‚æ•°é‡: 2834685952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10731520 || æ€»çš„å‚æ•°é‡: 2834718720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10731520 || æ€»çš„å‚æ•°é‡: 2836815872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10764288 || æ€»çš„å‚æ•°é‡: 2836848640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10772480 || æ€»çš„å‚æ•°é‡: 2836856832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10772480 || æ€»çš„å‚æ•°é‡: 2838953984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10805248 || æ€»çš„å‚æ•°é‡: 2838986752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10813440 || æ€»çš„å‚æ•°é‡: 2838994944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10813440 || æ€»çš„å‚æ•°é‡: 2847383552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10846208 || æ€»çš„å‚æ•°é‡: 2847416320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10878976 || æ€»çš„å‚æ•°é‡: 2847449088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10878976 || æ€»çš„å‚æ•°é‡: 2876809216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 10911744 || æ€»çš„å‚æ•°é‡: 2876841984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11026432 || æ€»çš„å‚æ•°é‡: 2876956672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11026432 || æ€»çš„å‚æ•°é‡: 2906316800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11059200 || æ€»çš„å‚æ•°é‡: 2906349568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11173888 || æ€»çš„å‚æ•°é‡: 2906464256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11173888 || æ€»çš„å‚æ•°é‡: 2935824384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11173888 || æ€»çš„å‚æ•°é‡: 2935828480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11173888 || æ€»çš„å‚æ•°é‡: 2935832576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11173888 || æ€»çš„å‚æ•°é‡: 2944221184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11206656 || æ€»çš„å‚æ•°é‡: 2944253952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11239424 || æ€»çš„å‚æ•°é‡: 2944286720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11239424 || æ€»çš„å‚æ•°é‡: 2946383872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11272192 || æ€»çš„å‚æ•°é‡: 2946416640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11280384 || æ€»çš„å‚æ•°é‡: 2946424832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11280384 || æ€»çš„å‚æ•°é‡: 2948521984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11313152 || æ€»çš„å‚æ•°é‡: 2948554752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11321344 || æ€»çš„å‚æ•°é‡: 2948562944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11321344 || æ€»çš„å‚æ•°é‡: 2956951552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11354112 || æ€»çš„å‚æ•°é‡: 2956984320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11386880 || æ€»çš„å‚æ•°é‡: 2957017088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11386880 || æ€»çš„å‚æ•°é‡: 2986377216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11419648 || æ€»çš„å‚æ•°é‡: 2986409984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11534336 || æ€»çš„å‚æ•°é‡: 2986524672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11534336 || æ€»çš„å‚æ•°é‡: 3015884800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11567104 || æ€»çš„å‚æ•°é‡: 3015917568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11681792 || æ€»çš„å‚æ•°é‡: 3016032256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11681792 || æ€»çš„å‚æ•°é‡: 3045392384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11681792 || æ€»çš„å‚æ•°é‡: 3045396480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11681792 || æ€»çš„å‚æ•°é‡: 3045400576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11681792 || æ€»çš„å‚æ•°é‡: 3053789184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11714560 || æ€»çš„å‚æ•°é‡: 3053821952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11747328 || æ€»çš„å‚æ•°é‡: 3053854720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11747328 || æ€»çš„å‚æ•°é‡: 3055951872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11780096 || æ€»çš„å‚æ•°é‡: 3055984640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11788288 || æ€»çš„å‚æ•°é‡: 3055992832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11788288 || æ€»çš„å‚æ•°é‡: 3058089984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11821056 || æ€»çš„å‚æ•°é‡: 3058122752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11829248 || æ€»çš„å‚æ•°é‡: 3058130944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11829248 || æ€»çš„å‚æ•°é‡: 3066519552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11862016 || æ€»çš„å‚æ•°é‡: 3066552320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11894784 || æ€»çš„å‚æ•°é‡: 3066585088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 11894784 || æ€»çš„å‚æ•°é‡: 3095945216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.38\n",
      "è®­ç»ƒå‚æ•°é‡: 11927552 || æ€»çš„å‚æ•°é‡: 3095977984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12042240 || æ€»çš„å‚æ•°é‡: 3096092672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12042240 || æ€»çš„å‚æ•°é‡: 3125452800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12075008 || æ€»çš„å‚æ•°é‡: 3125485568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12189696 || æ€»çš„å‚æ•°é‡: 3125600256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12189696 || æ€»çš„å‚æ•°é‡: 3154960384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12189696 || æ€»çš„å‚æ•°é‡: 3154964480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12189696 || æ€»çš„å‚æ•°é‡: 3154968576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12189696 || æ€»çš„å‚æ•°é‡: 3163357184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12222464 || æ€»çš„å‚æ•°é‡: 3163389952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12255232 || æ€»çš„å‚æ•°é‡: 3163422720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12255232 || æ€»çš„å‚æ•°é‡: 3165519872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12288000 || æ€»çš„å‚æ•°é‡: 3165552640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12296192 || æ€»çš„å‚æ•°é‡: 3165560832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12296192 || æ€»çš„å‚æ•°é‡: 3167657984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12328960 || æ€»çš„å‚æ•°é‡: 3167690752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12337152 || æ€»çš„å‚æ•°é‡: 3167698944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12337152 || æ€»çš„å‚æ•°é‡: 3176087552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12369920 || æ€»çš„å‚æ•°é‡: 3176120320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12402688 || æ€»çš„å‚æ•°é‡: 3176153088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12402688 || æ€»çš„å‚æ•°é‡: 3205513216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12435456 || æ€»çš„å‚æ•°é‡: 3205545984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12550144 || æ€»çš„å‚æ•°é‡: 3205660672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12550144 || æ€»çš„å‚æ•°é‡: 3235020800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12582912 || æ€»çš„å‚æ•°é‡: 3235053568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12697600 || æ€»çš„å‚æ•°é‡: 3235168256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12697600 || æ€»çš„å‚æ•°é‡: 3264528384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12697600 || æ€»çš„å‚æ•°é‡: 3264532480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12697600 || æ€»çš„å‚æ•°é‡: 3264536576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12697600 || æ€»çš„å‚æ•°é‡: 3272925184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12730368 || æ€»çš„å‚æ•°é‡: 3272957952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12763136 || æ€»çš„å‚æ•°é‡: 3272990720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12763136 || æ€»çš„å‚æ•°é‡: 3275087872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12795904 || æ€»çš„å‚æ•°é‡: 3275120640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12804096 || æ€»çš„å‚æ•°é‡: 3275128832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12804096 || æ€»çš„å‚æ•°é‡: 3277225984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12836864 || æ€»çš„å‚æ•°é‡: 3277258752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12845056 || æ€»çš„å‚æ•°é‡: 3277266944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12845056 || æ€»çš„å‚æ•°é‡: 3285655552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12877824 || æ€»çš„å‚æ•°é‡: 3285688320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12910592 || æ€»çš„å‚æ•°é‡: 3285721088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12910592 || æ€»çš„å‚æ•°é‡: 3315081216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 12943360 || æ€»çš„å‚æ•°é‡: 3315113984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13058048 || æ€»çš„å‚æ•°é‡: 3315228672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13058048 || æ€»çš„å‚æ•°é‡: 3344588800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13090816 || æ€»çš„å‚æ•°é‡: 3344621568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13205504 || æ€»çš„å‚æ•°é‡: 3344736256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13205504 || æ€»çš„å‚æ•°é‡: 3374096384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13205504 || æ€»çš„å‚æ•°é‡: 3374100480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13205504 || æ€»çš„å‚æ•°é‡: 3374104576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13205504 || æ€»çš„å‚æ•°é‡: 3382493184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13238272 || æ€»çš„å‚æ•°é‡: 3382525952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13271040 || æ€»çš„å‚æ•°é‡: 3382558720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13271040 || æ€»çš„å‚æ•°é‡: 3384655872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13303808 || æ€»çš„å‚æ•°é‡: 3384688640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13312000 || æ€»çš„å‚æ•°é‡: 3384696832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13312000 || æ€»çš„å‚æ•°é‡: 3386793984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13344768 || æ€»çš„å‚æ•°é‡: 3386826752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13352960 || æ€»çš„å‚æ•°é‡: 3386834944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13352960 || æ€»çš„å‚æ•°é‡: 3395223552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13385728 || æ€»çš„å‚æ•°é‡: 3395256320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13418496 || æ€»çš„å‚æ•°é‡: 3395289088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13418496 || æ€»çš„å‚æ•°é‡: 3424649216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13451264 || æ€»çš„å‚æ•°é‡: 3424681984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13565952 || æ€»çš„å‚æ•°é‡: 3424796672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13565952 || æ€»çš„å‚æ•°é‡: 3454156800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13598720 || æ€»çš„å‚æ•°é‡: 3454189568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13713408 || æ€»çš„å‚æ•°é‡: 3454304256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13713408 || æ€»çš„å‚æ•°é‡: 3483664384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13713408 || æ€»çš„å‚æ•°é‡: 3483668480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13713408 || æ€»çš„å‚æ•°é‡: 3483672576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13713408 || æ€»çš„å‚æ•°é‡: 3492061184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13746176 || æ€»çš„å‚æ•°é‡: 3492093952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13778944 || æ€»çš„å‚æ•°é‡: 3492126720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13778944 || æ€»çš„å‚æ•°é‡: 3494223872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13811712 || æ€»çš„å‚æ•°é‡: 3494256640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13819904 || æ€»çš„å‚æ•°é‡: 3494264832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13819904 || æ€»çš„å‚æ•°é‡: 3496361984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13852672 || æ€»çš„å‚æ•°é‡: 3496394752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13860864 || æ€»çš„å‚æ•°é‡: 3496402944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13860864 || æ€»çš„å‚æ•°é‡: 3504791552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13893632 || æ€»çš„å‚æ•°é‡: 3504824320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13926400 || æ€»çš„å‚æ•°é‡: 3504857088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 13926400 || æ€»çš„å‚æ•°é‡: 3534217216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 13959168 || æ€»çš„å‚æ•°é‡: 3534249984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 14073856 || æ€»çš„å‚æ•°é‡: 3534364672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14073856 || æ€»çš„å‚æ•°é‡: 3563724800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 14106624 || æ€»çš„å‚æ•°é‡: 3563757568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14221312 || æ€»çš„å‚æ•°é‡: 3563872256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14221312 || æ€»çš„å‚æ•°é‡: 3593232384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14221312 || æ€»çš„å‚æ•°é‡: 3593236480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14221312 || æ€»çš„å‚æ•°é‡: 3593240576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14221312 || æ€»çš„å‚æ•°é‡: 3601629184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.39\n",
      "è®­ç»ƒå‚æ•°é‡: 14254080 || æ€»çš„å‚æ•°é‡: 3601661952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14286848 || æ€»çš„å‚æ•°é‡: 3601694720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14286848 || æ€»çš„å‚æ•°é‡: 3603791872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14319616 || æ€»çš„å‚æ•°é‡: 3603824640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14327808 || æ€»çš„å‚æ•°é‡: 3603832832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14327808 || æ€»çš„å‚æ•°é‡: 3605929984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14360576 || æ€»çš„å‚æ•°é‡: 3605962752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14368768 || æ€»çš„å‚æ•°é‡: 3605970944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14368768 || æ€»çš„å‚æ•°é‡: 3614359552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14401536 || æ€»çš„å‚æ•°é‡: 3614392320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14434304 || æ€»çš„å‚æ•°é‡: 3614425088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14434304 || æ€»çš„å‚æ•°é‡: 3643785216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14467072 || æ€»çš„å‚æ•°é‡: 3643817984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14581760 || æ€»çš„å‚æ•°é‡: 3643932672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14581760 || æ€»çš„å‚æ•°é‡: 3673292800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14614528 || æ€»çš„å‚æ•°é‡: 3673325568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14729216 || æ€»çš„å‚æ•°é‡: 3673440256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14729216 || æ€»çš„å‚æ•°é‡: 3702800384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14729216 || æ€»çš„å‚æ•°é‡: 3702804480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14729216 || æ€»çš„å‚æ•°é‡: 3702808576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14729216 || æ€»çš„å‚æ•°é‡: 3711197184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14761984 || æ€»çš„å‚æ•°é‡: 3711229952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14794752 || æ€»çš„å‚æ•°é‡: 3711262720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14794752 || æ€»çš„å‚æ•°é‡: 3713359872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14827520 || æ€»çš„å‚æ•°é‡: 3713392640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14835712 || æ€»çš„å‚æ•°é‡: 3713400832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14835712 || æ€»çš„å‚æ•°é‡: 3715497984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14868480 || æ€»çš„å‚æ•°é‡: 3715530752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14876672 || æ€»çš„å‚æ•°é‡: 3715538944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14876672 || æ€»çš„å‚æ•°é‡: 3723927552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14909440 || æ€»çš„å‚æ•°é‡: 3723960320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14942208 || æ€»çš„å‚æ•°é‡: 3723993088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14942208 || æ€»çš„å‚æ•°é‡: 3753353216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 14974976 || æ€»çš„å‚æ•°é‡: 3753385984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15089664 || æ€»çš„å‚æ•°é‡: 3753500672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15089664 || æ€»çš„å‚æ•°é‡: 3782860800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15122432 || æ€»çš„å‚æ•°é‡: 3782893568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15237120 || æ€»çš„å‚æ•°é‡: 3783008256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15237120 || æ€»çš„å‚æ•°é‡: 3812368384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15237120 || æ€»çš„å‚æ•°é‡: 3812372480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15237120 || æ€»çš„å‚æ•°é‡: 3812376576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15237120 || æ€»çš„å‚æ•°é‡: 3820765184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15269888 || æ€»çš„å‚æ•°é‡: 3820797952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15302656 || æ€»çš„å‚æ•°é‡: 3820830720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15302656 || æ€»çš„å‚æ•°é‡: 3822927872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15335424 || æ€»çš„å‚æ•°é‡: 3822960640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15343616 || æ€»çš„å‚æ•°é‡: 3822968832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15343616 || æ€»çš„å‚æ•°é‡: 3825065984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15376384 || æ€»çš„å‚æ•°é‡: 3825098752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15384576 || æ€»çš„å‚æ•°é‡: 3825106944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15384576 || æ€»çš„å‚æ•°é‡: 3833495552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15417344 || æ€»çš„å‚æ•°é‡: 3833528320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15450112 || æ€»çš„å‚æ•°é‡: 3833561088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15450112 || æ€»çš„å‚æ•°é‡: 3862921216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15482880 || æ€»çš„å‚æ•°é‡: 3862953984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15597568 || æ€»çš„å‚æ•°é‡: 3863068672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15597568 || æ€»çš„å‚æ•°é‡: 3892428800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15630336 || æ€»çš„å‚æ•°é‡: 3892461568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15745024 || æ€»çš„å‚æ•°é‡: 3892576256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15745024 || æ€»çš„å‚æ•°é‡: 3921936384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15745024 || æ€»çš„å‚æ•°é‡: 3921940480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15745024 || æ€»çš„å‚æ•°é‡: 3921944576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15745024 || æ€»çš„å‚æ•°é‡: 3930333184 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15777792 || æ€»çš„å‚æ•°é‡: 3930365952 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15810560 || æ€»çš„å‚æ•°é‡: 3930398720 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15810560 || æ€»çš„å‚æ•°é‡: 3932495872 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15843328 || æ€»çš„å‚æ•°é‡: 3932528640 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15851520 || æ€»çš„å‚æ•°é‡: 3932536832 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15851520 || æ€»çš„å‚æ•°é‡: 3934633984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15884288 || æ€»çš„å‚æ•°é‡: 3934666752 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15892480 || æ€»çš„å‚æ•°é‡: 3934674944 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15892480 || æ€»çš„å‚æ•°é‡: 3943063552 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15925248 || æ€»çš„å‚æ•°é‡: 3943096320 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15958016 || æ€»çš„å‚æ•°é‡: 3943129088 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15958016 || æ€»çš„å‚æ•°é‡: 3972489216 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 15990784 || æ€»çš„å‚æ•°é‡: 3972521984 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16105472 || æ€»çš„å‚æ•°é‡: 3972636672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.41\n",
      "è®­ç»ƒå‚æ•°é‡: 16105472 || æ€»çš„å‚æ•°é‡: 4001996800 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16138240 || æ€»çš„å‚æ•°é‡: 4002029568 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4002144256 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.41\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4031504384 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4031508480 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4031512576 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4031516672 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.40\n",
      "è®­ç»ƒå‚æ•°é‡: 16252928 || æ€»çš„å‚æ•°é‡: 4556853248 || è®­ç»ƒå‚æ•°é‡å æ¯”%: 0.36\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model,peft_config)\n",
    "\n",
    "# è®¡ç®—å¯è®­ç»ƒæ•°é‡\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f8379-3e19-48d4-9559-47285c4f1c2f",
   "metadata": {},
   "source": [
    "# 5. ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e218befc-4110-45ad-9d55-4d58a7f7a326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/anaconda3/envs/llama/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/leon/projects/models/Meta-Llama-3-8B/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-puddle-2</strong> at: <a href='https://wandb.ai/leon-2003ub313/test_fine_tuning/runs/8xhdr2ge' target=\"_blank\">https://wandb.ai/leon-2003ub313/test_fine_tuning/runs/8xhdr2ge</a><br/> View project at: <a href='https://wandb.ai/leon-2003ub313/test_fine_tuning' target=\"_blank\">https://wandb.ai/leon-2003ub313/test_fine_tuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240727_174700-8xhdr2ge/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿å­˜å¾®è°ƒæ¨¡å‹\n",
    "\n",
    "trainer.model.save_pretrained(\"/home/leon/projects/models/autodl-tmp/\")\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478b265-dd5d-42ae-a15f-4d8718480163",
   "metadata": {},
   "source": [
    "# 6. æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97948647-8724-41cf-8c01-a60f2fffe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseæ¨¡å‹æµ‹è¯•\n",
    "\n",
    "def stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt = \"Below is an instruction that describes a task. Write a response that appropriately comple\"\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=128) # è¿™ä¸ªmodelæ˜¯åŸå§‹æ¨¡å‹ï¼Œä¸æ˜¯å¾®è°ƒåçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d1ab4f1-e548-4d59-918f-c3fc550c2fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®‰å€æ™‹ä¸‰,æ—¥æœ¬é¦–ç›¸ã€‚\n",
      "\n",
      "### Explanation:\n",
      "The sentence â€œå®‰å€æ™‹ä¸‰,æ—¥æœ¬é¦–ç›¸ã€‚â€ is a simple sentence. It is a statement that tells who the person is. The sentence contains one independent clause, which is the main idea of the sentence. The subject is å®‰å€æ™‹ä¸‰, and the predicate is æ—¥æœ¬é¦–ç›¸. The subject and the predicate are separated by a comma.\n"
     ]
    }
   ],
   "source": [
    "stream(\"å®‰å€æ˜¯è°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786ba3a-9304-4d78-9cc2-ab389fc62754",
   "metadata": {},
   "source": [
    "# 7. æ¨¡å‹åˆå¹¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a0f3439-292c-4271-b1c6-c2ac19e50118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32280980dad54e0eb7be7b05057c1ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# åˆå¹¶base model ä¸ lora model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/docs/trl/main/en/use_model#use-adapters-peft\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/modeling_utils.py:3838\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3829\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3831\u001b[0m     (\n\u001b[1;32m   3832\u001b[0m         model,\n\u001b[1;32m   3833\u001b[0m         missing_keys,\n\u001b[1;32m   3834\u001b[0m         unexpected_keys,\n\u001b[1;32m   3835\u001b[0m         mismatched_keys,\n\u001b[1;32m   3836\u001b[0m         offload_index,\n\u001b[1;32m   3837\u001b[0m         error_msgs,\n\u001b[0;32m-> 3838\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3849\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/modeling_utils.py:4298\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4294\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4295\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4296\u001b[0m                 )\n\u001b[1;32m   4297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4298\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4305\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4306\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4313\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4315\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/transformers/modeling_utils.py:895\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    884\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    893\u001b[0m ):\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.8/site-packages/accelerate/utils/modeling.py:404\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    402\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 404\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# åˆå¹¶base model ä¸ lora model\n",
    "# https://huggingface.co/docs/trl/main/en/use_model#use-adapters-peft\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "754da922-7f4e-40aa-b795-42ce3c62c842",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mbase_model\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/leon/projects/models/autodl-tmp/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "new_model = PeftModel.from_pretrained(base_model, \"/home/leon/projects/models/autodl-tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c9e7b14-dd1d-4909-ad5c-ab8ab4174621",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# æ¨¡å‹åˆå¹¶\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹åˆå¹¶\n",
    "merged_model = new_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt = \"Below is an instruction that describes a task. Write a response that appropriately comple\"\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = merged_model.generate(**inputs, streamer=streamer, max_new_tokens=128,num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa844b51-143d-4779-81a0-303ac7a96e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
